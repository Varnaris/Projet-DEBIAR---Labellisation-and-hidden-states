{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "KXQltZsTDd63",
   "metadata": {
    "id": "KXQltZsTDd63"
   },
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6b916c7",
   "metadata": {
    "id": "b6b916c7"
   },
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8162cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug  7 13:32:08 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   0  GRID A100D-20C                 On  |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   N/A    P0             N/A /  N/A  |   12609MiB /  20480MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   1938202      C   /home/evuichard/gpuenv/bin/python           11953MiB |\n",
      "|    0   N/A  N/A   1942073      C   /home/evuichard/gpuenv/bin/python             655MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "\"\"\"import torch\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\"\"\"\n",
    "!nvidia-smi\n",
    "\n",
    "#!pkill -u $USER -f python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "acXFHcPiRs9Y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4832,
     "status": "ok",
     "timestamp": 1751915534514,
     "user": {
      "displayName": "Elouan",
      "userId": "04663354146870278787"
     },
     "user_tz": -120
    },
    "id": "acXFHcPiRs9Y",
    "outputId": "878177e0-39c1-4560-f9e1-dd77dcdefbe1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f06bacce864334960767dc9204bb2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8253bfcb76d947d780cd353f0d7300e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773a4f84f9844b349823298d35a16711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e0c71057354a28b48c9d325bfc6847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4462149a544baa90fa496602b42a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/80.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11654dd43aab47e0b13b11e1fba0e4f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/367 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ef0f04d30d450ba28afefe67adee1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce8026e28fb4256858e01a613c868e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ecd46b823d4e06885f914bb5aa9fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/266 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7615616512\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"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\")\n",
    "\n",
    "import bitsandbytes\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "#clear cache\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#model_name = \"unsloth/Mistral-Small-Instruct-2409-bnb-4bit\"\n",
    "#model_name = \"unsloth/Phi-4-mini-instruct-bnb-4bit\"\n",
    "#model_name = \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit\"\n",
    "#model_name = \"unsloth/Qwen2-7B-Instruct-bnb-4bit\"\n",
    "#model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "\n",
    "\"\"\"bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\"\"\"\n",
    "\n",
    "#8bit quantization\n",
    "\"\"\"bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True, \n",
    "    bnb_8bit_use_double_quant=True,\n",
    "    bnb_8bit_quant_type=\"nf4\",\n",
    "    bnb_8bit_compute_dtype=torch.bfloat16\n",
    ")\"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    #quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "model.to(\"cuda:0\")\n",
    "\n",
    "print(model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bcb7a012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug  8 08:04:02 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  GRID A100D-20C                 On  |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   N/A    P0             N/A /  N/A  |   12611MiB /  20480MiB |      7%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   1938202      C   /home/evuichard/gpuenv/bin/python           11955MiB |\n",
      "|    0   N/A  N/A   1942073      C   /home/evuichard/gpuenv/bin/python             655MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JmBmko5YDI0g",
   "metadata": {
    "id": "JmBmko5YDI0g"
   },
   "source": [
    "# Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "s0U7QOB9i_lY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3928,
     "status": "ok",
     "timestamp": 1751915538444,
     "user": {
      "displayName": "Elouan",
      "userId": "04663354146870278787"
     },
     "user_tz": -120
    },
    "id": "s0U7QOB9i_lY",
    "outputId": "d854db09-202a-4e4a-88bf-42de6c4cbb20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the shape of the earth ? Answer :  The Earth is an oblate spheroid, which means it is mostly spherical but slightly flattened at the poles and bulging at the equator. This shape is caused by the centrifugal force of the Earth's rotation, which pushes matter away from\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"What is the shape of the earth ? Answer : \", return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "keOoJOOcla5e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1751915538459,
     "user": {
      "displayName": "Elouan",
      "userId": "04663354146870278787"
     },
     "user_tz": -120
    },
    "id": "keOoJOOcla5e",
    "outputId": "fc5e9de6-41c2-49f2-b26d-737ed5f49697"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'def getGenreReasonAndAttention(lyrics):\\n  prompt = prompt_gender(lyrics)\\n  inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\\n  # Generate output with attention\\n  output = model.generate(\\n    **inputs,\\n    max_new_tokens=100,\\n    do_sample=False,\\n    output_attentions=True,\\n    return_dict_in_generate=True\\n  )\\n  output_text = tokenizer.decode(output.sequences[0], skip_special_tokens=True)\\n  genre, keywords = getGenreAndKeywords(output_text)\\n  # Try to get cross-attentions (if available)\\n  cross_attentions = None\\n  if hasattr(output, \"cross_attentions\"):\\n    cross_attentions = output.cross_attentions\\n  return genre, keywords, cross_attentions'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "promptSize = 849 #303\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
    "\n",
    "\n",
    "# Prompt few-shot style CSV\n",
    "def prompt_gender(lyrics):\n",
    "  return f\"\"\"You are a gender classifier that labels song lyrics based on whether the narrator appears to be male, female, or neutral. Use lyrical content, tone, and perspective to decide. Your answer must include specific words or phrases from the lyrics that influenced your decision. Return the result using this format:\n",
    "\n",
    "LYRICS: <lyrics>  \n",
    "GENDER: <male|female|neutral>  \n",
    "KEYWORDS: <list of specific words or expressions from the lyrics>\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "LYRICS: I wear my heart upon my sleeve, like a girl who's never been hurt before  \n",
    "GENDER: female  \n",
    "KEYWORDS: \"girl\", \"wear my heart upon my sleeve\"\n",
    "\n",
    "LYRICS: Got my truck and my beer, ain't got no time for games  \n",
    "GENDER: male  \n",
    "KEYWORDS: \"truck\", \"beer\", \"ain't got no time\"\n",
    "\n",
    "LYRICS: The sky is open, my soul is light, I drift where the wind tells me to  \n",
    "GENDER: neutral  \n",
    "KEYWORDS: \"sky\", \"soul\", \"wind\"\n",
    "\n",
    "LYRICS: {lyrics}  \n",
    "GENDER:\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "def getGenreAndKeywords(result):\n",
    "  result = result[promptSize:]\n",
    "  match = re.search(\n",
    "    r\"GENDER:\\s*(male|female|neutral)\\s*KEYWORDS:\\s*([^\\n]+)\",\n",
    "    result,\n",
    "    re.IGNORECASE\n",
    "  )\n",
    "  if match:\n",
    "    gender = match.group(1)\n",
    "    keywords_str = match.group(2).strip()\n",
    "    keywords_list = [kw.strip().replace('\"', '').replace('\\'', '') for kw in keywords_str.split(',') if kw.strip().replace('\"', '').replace('\\'', '')]\n",
    "    keywords_list = list(set(keywords_list))  # Remove duplicates\n",
    "    return gender, keywords_list\n",
    "  else:\n",
    "    print(\"Impossible d'extraire le genre ou les mots-clés.\")\n",
    "    print(result)\n",
    "    return None, None\n",
    "\n",
    "def getGenreAndReasonLLM(lyrics):\n",
    "  prompt = prompt_gender(lyrics)\n",
    "  result = pipe(prompt, max_new_tokens=100, do_sample=False) # Removed temperature=0.7\n",
    "  output_text = result[0][\"generated_text\"]\n",
    "  return getGenreAndKeywords(output_text)\n",
    "\n",
    "\"\"\"def getGenreReasonAndAttention(lyrics):\n",
    "  prompt = prompt_gender(lyrics)\n",
    "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "  # Generate output with attention\n",
    "  output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=False,\n",
    "    output_attentions=True,\n",
    "    return_dict_in_generate=True\n",
    "  )\n",
    "  output_text = tokenizer.decode(output.sequences[0], skip_special_tokens=True)\n",
    "  genre, keywords = getGenreAndKeywords(output_text)\n",
    "  # Try to get cross-attentions (if available)\n",
    "  cross_attentions = None\n",
    "  if hasattr(output, \"cross_attentions\"):\n",
    "    cross_attentions = output.cross_attentions\n",
    "  return genre, keywords, cross_attentions\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "383af90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir le nom du modèle et le chemin de sortie\n",
    "modele_str = model_name.split('/')[-1]\n",
    "PATH_output_gender = f'/home/evuichard/Projet DEBIAR/labeled_lyrics_gender_{modele_str}.xlsx'\n",
    "\n",
    "# Vérifier si le fichier de sortie existe et s'il est correctement formaté\n",
    "def need_init(path):\n",
    "  if not os.path.isfile(path):\n",
    "    return True\n",
    "  df_check = pd.read_excel(path, sheet_name=\"Sheet1\")\n",
    "\n",
    "  if 'keywords_LLM_Gender' not in df_check.columns or df_check['keywords_LLM_Gender'].notna().sum() <= 200:\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "if need_init(PATH_output_gender):\n",
    "  df = pd.read_excel('/home/evuichard/Projet DEBIAR/translated_lyrics.xlsx', sheet_name=\"Sheet1\")\n",
    "  df['genre_LLM'] = None\n",
    "  df['keywords_LLM_Gender'] = None\n",
    "  df.to_excel(PATH_output_gender, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4BDAveL8s3gf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8521,
     "status": "ok",
     "timestamp": 1751915546995,
     "user": {
      "displayName": "Elouan",
      "userId": "04663354146870278787"
     },
     "user_tz": -120
    },
    "id": "4BDAveL8s3gf",
    "outputId": "925ad54e-5ce5-4eee-d22f-ce7d43393bed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('neutral', ['love', 'poison', 'girl'])\n"
     ]
    }
   ],
   "source": [
    "df = pd.ExcelFile(PATH_output_gender)\n",
    "df = df.parse(\"Sheet1\")\n",
    "print(getGenreAndReasonLLM(df['english_lyrics'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "AN1hGbroqZv0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 950806,
     "status": "ok",
     "timestamp": 1751916497813,
     "user": {
      "displayName": "Elouan",
      "userId": "04663354146870278787"
     },
     "user_tz": -120
    },
    "id": "AN1hGbroqZv0",
    "outputId": "55672fbd-f177-4a36-d59c-17555f6bfd04"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index: 1500 / 1502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500 neutral ['little grace LYRICS: <lyrics>', 'dance', 'bamba']\n",
      "1501 neutral ['deceiving', 'beautify LYRICS: Im not afraid to die', 'lying', 'bleeding', 'I just hope that youll be there', 'hatred']\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "df['genre_LLM'] = df['genre_LLM'].astype('object')\n",
    "df['keywords_LLM_Gender'] = df['keywords_LLM_Gender'].astype('object')\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "  \"\"\"if index > 200:\n",
    "    print(\"Temps limite atteint.\")\n",
    "    break\"\"\"\n",
    "  if index % 100 == 0:\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Processing index: {index} / {len(df)}\")\n",
    "  if len(str(df['genre_LLM'][index])) > 3:\n",
    "    print(index, end=' ')\n",
    "    continue\n",
    "  genre, keywords = getGenreAndReasonLLM(df['english_lyrics'][index])\n",
    "  df.loc[index, 'genre_LLM'] = genre\n",
    "  df.loc[index, 'keywords_LLM_Gender'] = \", \".join(keywords) if keywords else None\n",
    "  df.to_excel(PATH_output_gender)\n",
    "  print(index, genre, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fp15QK1wP2v5",
   "metadata": {
    "id": "fp15QK1wP2v5"
   },
   "outputs": [],
   "source": [
    "#mettre en minuscule tous les labels de genre\n",
    "df['genre_LLM'] = df['genre_LLM'].str.lower()\n",
    "df.to_excel(PATH_output_gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k3GQiEsbDN4a",
   "metadata": {
    "id": "k3GQiEsbDN4a"
   },
   "source": [
    "# Ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "gpX9byBEdces",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1751916499626,
     "user": {
      "displayName": "Elouan",
      "userId": "04663354146870278787"
     },
     "user_tz": -120
    },
    "id": "gpX9byBEdces",
    "outputId": "1a778e4d-5d2f-49e3-abf9-f0ce255a0610"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "849\n"
     ]
    }
   ],
   "source": [
    "print(len(\"\"\"You are a gender classifier that labels song lyrics based on whether the narrator appears to be male, female, or neutral. Use lyrical content, tone, and perspective to decide. Your answer must include specific words or phrases from the lyrics that influenced your decision. Return the result using this format:\n",
    "\n",
    "LYRICS: <lyrics>  \n",
    "GENDER: <male|female|neutral>  \n",
    "KEYWORDS: <list of specific words or expressions from the lyrics>\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "LYRICS: I wear my heart upon my sleeve, like a girl who's never been hurt before  \n",
    "GENDER: female  \n",
    "KEYWORDS: \"girl\", \"wear my heart upon my sleeve\"\n",
    "\n",
    "LYRICS: Got my truck and my beer, ain't got no time for games  \n",
    "GENDER: male  \n",
    "KEYWORDS: \"truck\", \"beer\", \"ain't got no time\"\n",
    "\n",
    "LYRICS: The sky is open, my soul is light, I drift where the wind tells me to  \n",
    "GENDER: neutral  \n",
    "KEYWORDS: \"sky\", \"soul\", \"wind\"\n",
    "\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3ZXtR3C3PyX6",
   "metadata": {
    "id": "3ZXtR3C3PyX6"
   },
   "outputs": [],
   "source": [
    "promptSize = 1042 #502\n",
    "def prompt_ethnicity(lyrics):\n",
    "  return f\"\"\"You are an ethnicity classifier. Based on a block of song lyrics written in English, determine the most likely cultural or ethnic background of the narrator. Use references to places, culture, language style, and musical or thematic elements.\n",
    "\n",
    "Choose only one of the following labels:\n",
    "- European\n",
    "- Asian\n",
    "- African\n",
    "- Arabic\n",
    "- North American\n",
    "- Latino / South American\n",
    "- Oceania\n",
    "\n",
    "Return the result in the following format:\n",
    "LYRICS: <lyrics>  \n",
    "ETHNICITY: <label>  \n",
    "KEYWORDS: <list of specific words or expressions from the lyrics that led to your decision>\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "LYRICS: From Lagos to the stage, I carry my father's name / Afrobeat in my soul, fire in my veins  \n",
    "ETHNICITY: African  \n",
    "KEYWORDS: \"Lagos\", \"Afrobeat\", \"father's name\"\n",
    "\n",
    "LYRICS: I grew up on ramen and neon lights / Tokyo dreams in sleepless nights  \n",
    "ETHNICITY: Asian  \n",
    "KEYWORDS: \"ramen\", \"Tokyo\", \"neon lights\"\n",
    "\n",
    "LYRICS: Raised on southern blues and faded jeans / Mama's voice and Georgia dreams  \n",
    "ETHNICITY: North American  \n",
    "KEYWORDS: \"southern blues\", \"faded jeans\", \"Georgia\"\n",
    "\n",
    "LYRICS: {lyrics}  \n",
    "ETHNICITY:\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "def getEthnicityAndKeywords(result):\n",
    "  result = result[promptSize:]\n",
    "  match = re.search(\n",
    "    r\"ETHNICITY:\\s*(European|Asian|African|Arabic|North American|Latino / South American|Oceania)\\s*KEYWORDS:\\s*([^\\n]+)\",\n",
    "    result,\n",
    "    re.IGNORECASE\n",
    "  )\n",
    "  if match:\n",
    "    ethnie = match.group(1)\n",
    "    keywords_str = match.group(2).strip()\n",
    "    # Remove quotes and split by comma\n",
    "    keywords_list = [kw.strip().replace('\"', '').replace('\\'', '') for kw in keywords_str.split(',') if kw.strip().replace('\"', '').replace('\\'', '')]\n",
    "    keywords_list = list(set(keywords_list))  # Remove duplicates\n",
    "    return ethnie, keywords_list\n",
    "  else:\n",
    "    print(\"Impossible d'extraire l'ethnie ou les mots-clés.\")\n",
    "    print(result)\n",
    "    return None, None\n",
    "\n",
    "def getEthnicityAndReasonLLM(lyrics):\n",
    "  prompt = prompt_ethnicity(lyrics)\n",
    "  result = pipe(prompt, max_new_tokens=100, do_sample=False) # Removed temperature=0.7\n",
    "  output_text = result[0][\"generated_text\"]\n",
    "  return getEthnicityAndKeywords(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "NmjEDDueSoIq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9297,
     "status": "ok",
     "timestamp": 1751916508934,
     "user": {
      "displayName": "Elouan",
      "userId": "04663354146870278787"
     },
     "user_tz": -120
    },
    "id": "NmjEDDueSoIq",
    "outputId": "6e5061b7-d5df-4ded-a331-3c6bcd7b927b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('African', ['Freeze', 'fathers name', 'Spyderman', 'Afrobeat', 'Tokyo', 'Poison (referring to African slang)', 'NA', 'ramen'])\n"
     ]
    }
   ],
   "source": [
    "df = pd.ExcelFile(PATH_output_gender)\n",
    "df = df.parse(\"Sheet1\")\n",
    "print(getEthnicityAndReasonLLM(df['english_lyrics'][0]))\n",
    "#if df don't already have 'ethnicity_LLM' key\n",
    "if 'ethnicity_LLM' not in df.columns or 'keywords_LLM_Ethicity' not in df.columns:\n",
    "  df['ethnicity_LLM'] = None\n",
    "  df['keywords_LLM_Ethicity'] = None\n",
    "  df.to_excel(PATH_output_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6LEEnPg6UGOK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1609300,
     "status": "ok",
     "timestamp": 1751918118261,
     "user": {
      "displayName": "Elouan",
      "userId": "04663354146870278787"
     },
     "user_tz": -120
    },
    "id": "6LEEnPg6UGOK",
    "outputId": "497545f1-56ee-4316-855f-dd1326234a12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index: 1500 / 1502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500 Latino / South American ['Georgia (repeated as a reference to the US South)', 'bamba', 'sailor', 'captain']\n",
      "1501 North American ['Georgia', 'deceiving', 'lying', 'southern blues', 'beautify', 'hatred']\n"
     ]
    }
   ],
   "source": [
    "df['ethnicity_LLM'] = df['ethnicity_LLM'].astype('object')\n",
    "df['keywords_LLM_Ethicity'] = df['keywords_LLM_Ethicity'].astype('object')\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "  \"\"\"if index > 200:\n",
    "    print(\"Temps limite atteint.\")\n",
    "    break\"\"\"\n",
    "  if index % 100 == 0:\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Processing index: {index} / {len(df)}\")\n",
    "  if len(str(df['ethnicity_LLM'][index])) > 3:\n",
    "    print(df['ethnicity_LLM'][index])\n",
    "    print(index, end=' ')\n",
    "    continue\n",
    "  genre, keywords = getEthnicityAndReasonLLM(df['english_lyrics'][index])\n",
    "  df.loc[index, 'ethnicity_LLM'] = genre\n",
    "  df.loc[index, 'keywords_LLM_Ethicity'] = \", \".join(keywords) if keywords else None\n",
    "  df.to_excel(PATH_output_gender)\n",
    "  print(index, genre, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "uCuiB-ZQylJn",
   "metadata": {
    "id": "uCuiB-ZQylJn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(PATH_output_gender)\n",
    "#on affiche le nom des artistes qui ont pour genre \"person\"\n",
    "print(df[df['genre'] == 'Person']['track_artist'].unique())\n",
    "#SAYMYNAME => female\n",
    "#Aleman => male\n",
    "#Fili => group\n",
    "#MiMS => male\n",
    "\n",
    "df.loc[df['track_artist'] == 'SAYMYNAME', 'genre'] = 'female'\n",
    "df.loc[df['track_artist'] == 'Aleman', 'genre'] = 'male'\n",
    "df.loc[df['track_artist'] == 'Fili', 'genre'] = 'group'\n",
    "df.loc[df['track_artist'] == 'MiMS', 'genre'] = 'male'\n",
    "df = df.dropna(subset=['genre'])\n",
    "df.to_excel(PATH_output_gender, index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "KXQltZsTDd63",
    "JmBmko5YDI0g"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (GPU ENV)",
   "language": "python",
   "name": "gpuenv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
