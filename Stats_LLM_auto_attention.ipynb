{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "KXQltZsTDd63",
   "metadata": {
    "id": "KXQltZsTDd63"
   },
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b916c7",
   "metadata": {
    "id": "b6b916c7"
   },
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "import pandas as pd\n",
    "import gc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8162cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import torch\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\"\"\"\n",
    "!nvidia-smi\n",
    "\n",
    "#!pkill -u $USER -f python\n",
    "#!kill 2097171       \n",
    "\"\"\"\n",
    "del tokenizer\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acXFHcPiRs9Y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4832,
     "status": "ok",
     "timestamp": 1751915534514,
     "user": {
      "displayName": "Elouan",
      "userId": "04663354146870278787"
     },
     "user_tz": -120
    },
    "id": "acXFHcPiRs9Y",
    "outputId": "878177e0-39c1-4560-f9e1-dd77dcdefbe1"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"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\")\n",
    "\n",
    "import bitsandbytes\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n",
    "\n",
    "model_name = \"unsloth/Mistral-Small-Instruct-2409-bnb-4bit\"\n",
    "#model_name = \"unsloth/Phi-4-mini-instruct-bnb-4bit\"\n",
    "#model_name = \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit\"\n",
    "#model_name = \"unsloth/Qwen2-7B-Instruct-bnb-4bit\"\n",
    "#model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\"\"\"config = AutoConfig.from_pretrained(model_name)\n",
    "config.attn_implementation = \"eager\"\n",
    "config.output_attentions = True\n",
    "config.output_hidden_states = False \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "model.to(\"cuda:0\")\n",
    "\n",
    "print(model.num_parameters())\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb7a012",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JmBmko5YDI0g",
   "metadata": {
    "id": "JmBmko5YDI0g"
   },
   "source": [
    "# Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406b595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "\n",
    "promptSize = 849  # longueur fixe pour découper le résultat\n",
    "max_prompt_tokens = 300  # limiter le nombre de tokens des lyrics pour économiser de la mémoire\n",
    "\n",
    "def prompt_gender(lyrics, tokenizer=None, max_prompt_tokens=None, device=\"cuda:0\"):\n",
    "    # Tronquer uniquement les lyrics si max_prompt_tokens défini\n",
    "    if max_prompt_tokens is not None and tokenizer is not None:\n",
    "        tokenized_lyrics = tokenizer(lyrics, return_tensors=\"pt\").input_ids[0]\n",
    "        tokenized_lyrics = tokenized_lyrics[:max_prompt_tokens].to(device)\n",
    "        lyrics = tokenizer.decode(tokenized_lyrics, skip_special_tokens=True)\n",
    "\n",
    "    return f\"\"\"You are a gender classifier that labels song lyrics based on whether the narrator appears to be male, female, or neutral. Use lyrical content, tone, and perspective to decide. Your answer must include specific words or phrases from the lyrics that influenced your decision. Return the result using this format:\n",
    "\n",
    "LYRICS: <lyrics>  \n",
    "GENDER: <male|female|neutral>  \n",
    "KEYWORDS: <list of specific words or expressions from the lyrics>\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "LYRICS: I wear my heart upon my sleeve, like a girl who's never been hurt before  \n",
    "GENDER: female  \n",
    "KEYWORDS: \"girl\", \"wear my heart upon my sleeve\"\n",
    "\n",
    "LYRICS: Got my truck and my beer, ain't got no time for games  \n",
    "GENDER: male  \n",
    "KEYWORDS: \"truck\", \"beer\", \"ain't got no time\"\n",
    "\n",
    "LYRICS: The sky is open, my soul is light, I drift where the wind tells me to  \n",
    "GENDER: neutral  \n",
    "KEYWORDS: \"sky\", \"soul\", \"wind\"\n",
    "\n",
    "LYRICS: {lyrics}  \n",
    "GENDER:\"\"\"\n",
    "\n",
    "def getGenre(result):\n",
    "    result = result[promptSize:]\n",
    "    match = re.search(r\"GENDER:\\s*(male|female|neutral)\", result, re.IGNORECASE)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def getGenreLLM_with_attention_and_hidden(lyrics, model, tokenizer, device=\"cuda:0\"):\n",
    "    prompt = prompt_gender(lyrics, tokenizer=tokenizer, max_prompt_tokens=max_prompt_tokens, device=device)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    model.config.attn_implementation = \"eager\"\n",
    "    model.config.output_attentions = True\n",
    "    model.config.output_hidden_states = True\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Forward sur le prompt initial\n",
    "        outputs = model(**inputs, output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "        attentions_prompt = [att.clone().detach().cpu() for att in outputs.attentions]\n",
    "        hidden_states_prompt_full = outputs.hidden_states[-1].clone().detach().cpu()\n",
    "\n",
    "        # Générer un seul token supplémentaire (greedy top-1)\n",
    "        last_token_logits = outputs.logits[:, -1, :]\n",
    "        next_token_id = torch.argmax(last_token_logits, dim=-1).unsqueeze(-1)\n",
    "        input_with_next = torch.cat([inputs[\"input_ids\"], next_token_id], dim=-1)\n",
    "\n",
    "        # Forward pour le prompt + token généré\n",
    "        outputs_next = model(input_ids=input_with_next, output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "        attentions_next = [att.clone().detach().cpu() for att in outputs_next.attentions]\n",
    "        hidden_states_next_full = outputs_next.hidden_states[-1].clone().detach().cpu()\n",
    "\n",
    "        # Extraire uniquement la partie correspondant aux derniers lyrics et au token généré\n",
    "        #prompt_without_lyrics = tokenizer(prompt[0:promptSize], return_tensors=\"pt\").input_ids.to(device)\n",
    "        #len_prompt = prompt_without_lyrics.shape[1]+ 1\n",
    "        \n",
    "        len_prompt = 260\n",
    "        hidden_states_prompt = hidden_states_prompt_full[:, len_prompt:, :]\n",
    "        hidden_states_next = hidden_states_next_full[:, len_prompt:, :]\n",
    "\n",
    "        # Décodage pour récupérer genre\n",
    "        generated_text = tokenizer.decode(input_with_next[0], skip_special_tokens=True)\n",
    "        gender = getGenre(generated_text)\n",
    "\n",
    "        #del prompt_without_lyrics\n",
    "        del outputs, outputs_next, inputs, input_with_next, next_token_id, hidden_states_prompt_full, hidden_states_next_full\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return gender, attentions_prompt, attentions_next, hidden_states_prompt, hidden_states_next, generated_text\n",
    "\n",
    "\"\"\"\n",
    "# --- Exemple d'utilisation ---\n",
    "torch.cuda.empty_cache()\n",
    "lyrics = \"NA Yeah, Spyderman and Freeze in full effect Uh-huh You ready, Ron? I'm ready You ready, Biv? I'm ready, Slick, are you? Oh, yeah, break it down NA Girl, I, must (warn you) I sense something strange in my mind Situation is (serious) Let's cure it cause we're running out of time It's oh, so (beautiful) Relationships they seem from the start It's all so (deadly) When love is not together from the heart It's drivin' me out of my mind! That's why it's HARD for me to find Can't get it out of my head! Miss her, kiss her, love her(Wrong move you're dead!) That girl is (poison)...Never trust a big butt and smile That girl is (poison)..(\"POISON!!\") NA (-caution) Before I start to meet a fly girl, you know? Cause in some (portions) You'll think she's the best thing in the world She's so - (fly) She'll drive you right out of your mind And steal your heart when you're blind Beware she's schemin', she'll make you think you're dreamin' YOU'LL fall in love and you'll be screamin', demon, HOO.. Poison, deadly, movin' in slow Lookin for a mellow fellow like DeVoe Gettin paid, laid, so better lay low Schemin on house, money, and the whole show The low pro ho she'll be cut like an aaa-FRO See what you're sayin', huh, she's a winner to you But I know she's a loser (How do you know?) Me and the crew used to do her! \"POISON!\" \"POISON!\" \"POISON!\" \"POISON!\" \"POISON!\" \"POISON!\" \"POISON!\" \"POISON! \"POISON!\" \"POISON!\" \"POISON!\" \"POISON! \"POISON!\" \"POISON!\" \"POISON!\" \"POISON! I was at the park, shake, breakin and takin 'em all And that night, I played the wall Checkin' out the fellas, the highs and lows Keepin' one eye open, still clockin' the hoes There was one particular girl that stood out from the rest Poison as can be, the high power chest Michael Biv here and I'm runnin' the show Bell, Biv DeVoe ..now you know! Yo, Slick, blow.. It's drivin' me out of my mind! That's why it's HARD for me to find Can't get it out of my head! Miss her, kiss her, love her(Wrong move you're dead!) That girl is (poison)...Never trust a big butt and smile That girl is (poison)..(\"POISON!!\") Yo' fellas, that was my end of.. You know what I'm sayin', Mike? Yeah, B.B.D. in full effect Yo', wassup to Ralph T and Johnny G And I can't forget about my boy, B. Brown And the whole NE crew Poison.. NA\"\n",
    "gender, attentions_prompt, attentions_next, hidden_states_prompt, hidden_states_next, generated_text = getGenreLLM_with_attention_and_hidden(lyrics, model, tokenizer)\n",
    "\n",
    "print(f\"Genre : {gender}\")\n",
    "print(f\"Generated text: {generated_text}\")\n",
    "print(\"Shape attentions couche dernière (prompt) :\", attentions_prompt)\n",
    "print(\"Shape attentions couche dernière (prompt + token) :\", attentions_next)\n",
    "print(\"Shape hidden_states dernière couche (lyrics) :\", hidden_states_prompt.shape)\n",
    "print(\"Shape hidden_states dernière couche (lyrics + token) :\", hidden_states_next.shape)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383af90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir le nom du modèle et le chemin de sortie\n",
    "\n",
    "modele_str = model_name.split('/')[-1]\n",
    "PATH_output_gender = f'/home/evuichard/Projet DEBIAR/labeled_lyrics_{modele_str}_attention&hidden_states.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6429c688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "def weight_to_ansi(weight):\n",
    "    if weight < 0.5:\n",
    "        ratio = weight / 0.5\n",
    "        r = int(200 * (1 - ratio) + 200 * ratio)\n",
    "        g = int(200 * (1 - ratio) + 0 * ratio)\n",
    "        b = int(200 * (1 - ratio) + 200 * ratio)\n",
    "    else:\n",
    "        ratio = (weight - 0.5) / 0.5\n",
    "        r = int(200 * (1 - ratio) + 255 * ratio)\n",
    "        g = 0\n",
    "        b = int(200 * (1 - ratio) + 0 * ratio)\n",
    "        \n",
    "    ansi_code = rgb_to_ansi(r, g, b)\n",
    "    return ansi_code\n",
    "\n",
    "def rgb_to_ansi(r, g, b):\n",
    "    \"\"\"\n",
    "    Convertit une couleur RGB en code ANSI 256.\n",
    "    \"\"\"\n",
    "    # On approxime à 6 niveaux (0-5)\n",
    "    r = int(r / 51)\n",
    "    g = int(g / 51)\n",
    "    b = int(b / 51)\n",
    "    return 16 + 36*r + 6*g + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e0a281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_token_to_word(index, tokenizer, tokens = [], prompt =\"\"):\n",
    "    if not tokens:\n",
    "        tokens = tokenizer.encode(prompt)\n",
    "    if abs(index) >= len(tokens):\n",
    "        return None\n",
    "    print(tokens)\n",
    "    return tokenizer.decode([tokens[index]])\n",
    "\n",
    "def index_word_to_token(word, tokenizer, tokens = [], prompt =\"\"):\n",
    "    if not tokens:\n",
    "        tokens = tokenizer.encode(prompt)\n",
    "    token = tokenizer.encode(word)\n",
    "    if token not in tokens:\n",
    "        return None\n",
    "    return tokens.index(token)\n",
    "\n",
    "def visualize_token_indices(prompt, tokenizer, line_width=100):\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    words = [tokenizer.decode([t]) for t in tokens]\n",
    "\n",
    "    current_words, current_indices = \"\", \"\"\n",
    "    char_count = 0\n",
    "\n",
    "    for i, w in enumerate(words):\n",
    "        # si le token contient un saut de ligne -> on affiche le bloc avant de repartir\n",
    "        if \"\\n\" in w:\n",
    "            word_display = colored(w.replace(\"\\n\", \"\"), 'green', attrs=['bold'])\n",
    "            current_words += word_display + \" \"\n",
    "            \n",
    "            idx_str = str(i)\n",
    "            padding = max(len(w.replace(\"\\n\", \"\")) - len(idx_str), 0)\n",
    "            current_indices += \" \" * padding + colored(idx_str, 'cyan', attrs=['bold']) + \" \"\n",
    "            \n",
    "            # impression du bloc courant\n",
    "            print(current_words.rstrip())\n",
    "            print(current_indices.rstrip())\n",
    "            print()  # ligne vide pour séparer\n",
    "\n",
    "            # réinitialiser\n",
    "            current_words, current_indices = \"\", \"\"\n",
    "            char_count = 0\n",
    "            continue\n",
    "\n",
    "        # ajout normal\n",
    "        word_display = colored(w, 'green', attrs=['bold'])\n",
    "        current_words += word_display + \" \"\n",
    "        idx_str = str(i)\n",
    "        padding = max(len(w) - len(idx_str), 0)\n",
    "        current_indices += \" \" * padding + colored(idx_str, 'cyan', attrs=['bold']) + \" \"\n",
    "\n",
    "        char_count += len(w) + 1\n",
    "        if char_count > line_width:\n",
    "            print(current_words.rstrip())\n",
    "            print(current_indices.rstrip())\n",
    "            print()\n",
    "            current_words, current_indices = \"\", \"\"\n",
    "            char_count = 0\n",
    "\n",
    "    # afficher le dernier bloc si non vide\n",
    "    if current_words.strip():\n",
    "        print(current_words.rstrip())\n",
    "        print(current_indices.rstrip())\n",
    "\n",
    "\n",
    "\n",
    "visualize_token_indices(prompt_gender(\"\"), tokenizer)\n",
    "\n",
    "def visualize_weight_sentence(tokenizer, tokens, weights):\n",
    "    \"\"\"\n",
    "    Affiche une phrase avec les tokens colorés suivant leur poids d'importance.\n",
    "    Dégradé continu du gris clair au rouge vif.\n",
    "    \"\"\"\n",
    "    print(len(tokens), len(weights))\n",
    "    if not isinstance(weights, torch.Tensor):\n",
    "        weights = torch.tensor(weights)\n",
    "\n",
    "    # Normalisation des poids\n",
    "    weights = (weights - weights.mean()) / (weights.std() + 1e-5)  # z-score\n",
    "    weights = (weights - weights.min()) / (weights.max() - weights.min() + 1e-5)\n",
    "\n",
    "    spectrum_steps = 10\n",
    "    print(\"Spectre d'importance :\")\n",
    "    for i in range(spectrum_steps):\n",
    "        weight = i / (spectrum_steps - 1)\n",
    "        ansi_code = weight_to_ansi(weight)\n",
    "        print(f\"\\033[48;5;{ansi_code}m \\033[0m\", end='')\n",
    "    print(\"\\n\") \n",
    "    \n",
    "    # Décoder les tokens\n",
    "    decoded_tokens = tokenizer.convert_ids_to_tokens(tokens)\n",
    "    \n",
    "    for token, weight in zip(decoded_tokens, weights):\n",
    "        # Couleur : du gris clair (200,200,200) au rouge vif (255,0,0)\n",
    "        ansi_code = weight_to_ansi(weight)\n",
    "        print(f\"\\033[38;5;{ansi_code}m{token}\\033[0m\", end='')\n",
    "    print()\n",
    "\n",
    "tokens = tokenizer.encode(prompt_gender(\"\")[:50])\n",
    "visualize_weight_sentence(tokenizer, tokens, np.random.rand(len(tokens)))\n",
    "\n",
    "def get_most_important_tokens(tokenizer, tokens, weights, top_n=5):\n",
    "    \"\"\"\n",
    "    Récupère les tokens les plus importants en fonction de leurs poids.\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtenir les indices des tokens les plus importants\n",
    "    most_important_indices = weights.topk(top_n).indices\n",
    "\n",
    "    # S'assurer que tokens est un tensor ou utiliser une compréhension de liste\n",
    "    if isinstance(tokens, list):\n",
    "        selected_tokens = [tokens[i.item()] for i in most_important_indices]\n",
    "    else:\n",
    "        selected_tokens = tokens[most_important_indices]\n",
    "\n",
    "    # Décoder les tokens\n",
    "    most_important_tokens = tokenizer.convert_ids_to_tokens(selected_tokens)\n",
    "    return most_important_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba984acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "\n",
    "def visualize_attention_html(tokenizer, attention, tokens):\n",
    "    \"\"\"\n",
    "    Affiche une matrice d'attention en HTML.\n",
    "    \n",
    "    tokenizer : tokenizer pour décoder les tokens\n",
    "    attention : np.array ou torch.Tensor de shape (num_layers, num_heads, seq_len, seq_len)\n",
    "    tokens : tensor ou liste des tokens d'entrée\n",
    "    \n",
    "    layer : couche de la matrice à visualiser\n",
    "    head : tête à visualiser\n",
    "    \"\"\"\n",
    "    # Si torch tensor, convertir en numpy\n",
    "    if 'torch' in str(type(attention)):\n",
    "        attention = attention.detach().cpu().numpy()\n",
    "    \n",
    "    # Sélection de la couche et tête\n",
    "    att_matrix = attention\n",
    "    \n",
    "    # Décoder les tokens\n",
    "    decoded_tokens = tokenizer.convert_ids_to_tokens(tokens)\n",
    "    \n",
    "    # Normalisation pour couleur\n",
    "    att_norm = (att_matrix - att_matrix.min()) / (att_matrix.max() - att_matrix.min() + 1e-8)\n",
    "    \n",
    "    # Générer tableau HTML\n",
    "    html = \"<table style='border-collapse: collapse;'>\"\n",
    "    \n",
    "    # Première ligne : tokens en colonnes\n",
    "    html += \"<tr><th></th>\"\n",
    "    for tok in decoded_tokens:\n",
    "        html += f\"<th style='padding:2px; font-family:monospace;'>{tok}</th>\"\n",
    "    html += \"</tr>\"\n",
    "    \n",
    "    # Lignes de la matrice\n",
    "    for i, tok_row in enumerate(decoded_tokens):\n",
    "        html += f\"<tr><th style='padding:2px; font-family:monospace;'>{tok_row}</th>\"\n",
    "        for j in range(len(decoded_tokens)):\n",
    "            val = att_norm[i, j]\n",
    "            # Dégradé bleu clair → bleu foncé\n",
    "            r = int(255*(1-val))\n",
    "            g = int(255*(1-val))\n",
    "            b = int(255*val)\n",
    "            color = f'rgb({r},{g},{b})'\n",
    "            html += f\"<td style='background-color:{color}; width:20px; height:20px; text-align:center;'>{val:.2f}</td>\"\n",
    "        html += \"</tr>\"\n",
    "    \n",
    "    html += \"</table>\"\n",
    "    \n",
    "    display(HTML(html))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa03f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def PCA_on_dim_n(tensor, dim=0, n_espace_propre=0):\n",
    "    if tensor.dim() <= dim:\n",
    "        raise ValueError(\"La dimension spécifiée est supérieure à la dimension du tenseur.\")\n",
    "    permute_order = list(range(tensor.dim()))\n",
    "    permute_order.append(permute_order.pop(dim))  \n",
    "    tensor_perm = tensor.permute(*permute_order)  # shape: (..., target_dim)\n",
    "    shape_perm = tensor_perm.shape\n",
    "    tensor_reshaped = tensor_perm.reshape(-1, shape_perm[-1]).detach().cpu().numpy()\n",
    "    \n",
    "    pca = PCA()\n",
    "    pca.fit(tensor_reshaped)\n",
    "    \n",
    "    if n_espace_propre >= pca.components_.shape[0]:\n",
    "        raise ValueError(f\"n_espace_propre={n_espace_propre} dépasse le nombre de composantes ({pca.components_.shape[0]})\")\n",
    "\n",
    "    explained_var = pca.explained_variance_ratio_[n_espace_propre] * 100\n",
    "    print(f\"Composante {n_espace_propre} : {explained_var:.2f}% de la variance totale expliquée\")\n",
    "\n",
    "    reduced = np.dot(tensor_reshaped, pca.components_[n_espace_propre].T).reshape(*shape_perm[:-1], 1)\n",
    "    reduced_torch = torch.from_numpy(reduced).to(tensor.device)\n",
    "    \n",
    "    inv_permute = [0] * len(permute_order)\n",
    "    for i, j in enumerate(permute_order):\n",
    "        inv_permute[j] = i\n",
    "    reduced_torch = reduced_torch.permute(*inv_permute)\n",
    "\n",
    "    return reduced_torch.squeeze(dim)\n",
    "\n",
    "def somme_pondéré_PCA_valeur_propre(tensor, dim=0, n_espace_propre_max=0):\n",
    "    if tensor.dim() <= dim:\n",
    "        raise ValueError(\"La dimension spécifiée est supérieure à la dimension du tenseur.\")\n",
    "    permute_order = list(range(tensor.dim()))\n",
    "    permute_order.append(permute_order.pop(dim))  \n",
    "    tensor_perm = tensor.permute(*permute_order)  # shape: (..., target_dim)\n",
    "    shape_perm = tensor_perm.shape\n",
    "    tensor_reshaped = tensor_perm.reshape(-1, shape_perm[-1]).detach().cpu().numpy()\n",
    "    \n",
    "    pca = PCA()\n",
    "    pca.fit(tensor_reshaped)\n",
    "    \n",
    "    if n_espace_propre_max >= pca.components_.shape[0]:\n",
    "        raise ValueError(f\"n_espace_propre={n_espace_propre_max} dépasse le nombre de composantes ({pca.components_.shape[0]})\")\n",
    "\n",
    "    sum_reduced = 0\n",
    "    sum_valeur_propre = sum([pca.singular_values_[i] for i in range(n_espace_propre_max + 1)])\n",
    "    for i in range(n_espace_propre_max + 1):\n",
    "        reduced = np.dot(tensor_reshaped, pca.components_[n_espace_propre_max].T).reshape(*shape_perm[:-1], 1)\n",
    "        reduced_torch = torch.from_numpy(reduced).to(tensor.device) * pca.singular_values_[i] / sum_valeur_propre\n",
    "        sum_reduced += reduced_torch\n",
    "    inv_permute = [0] * len(permute_order)\n",
    "    \n",
    "    for i, j in enumerate(permute_order):\n",
    "        inv_permute[j] = i\n",
    "    sum_reduced = sum_reduced.permute(*inv_permute)\n",
    "\n",
    "    return sum_reduced.squeeze(dim)\n",
    "\n",
    "#### Weight - methods ####\n",
    "## vertical\n",
    "def weight_vertical_idToken(attention, idToken):\n",
    "    # attention : (seq_len, seq_len)\n",
    "    # output : tenseur de dimension (seq_len)\n",
    "    return attention[:, idToken]\n",
    "\n",
    "def weight_vertical_token(attention, idToken, tokens):\n",
    "    indices = [i for i, token in enumerate(tokens) if token == tokens[idToken]]\n",
    "    return attention[:, indices].mean(dim=1)\n",
    "\n",
    "def weight_vertical_mean(attention):\n",
    "    # attention : (seq_len, seq_len)\n",
    "    # output : tenseur de dimension (seq_len)\n",
    "    return attention.mean(dim=1)\n",
    "\n",
    "def weight_vertical_PCA(attention):\n",
    "    # attention : (seq_len,seq_len)\n",
    "    # Utilisation de sklearn pour faire une ACP sur l'axe 1 du tenseur\n",
    "    return PCA_on_dim_n(attention, dim=1)\n",
    "\n",
    "## horizontal\n",
    "def weight_horizontal_idToken(attention, idToken):\n",
    "    # attention : (seq_len, seq_len)\n",
    "    # output : tenseur de dimension (seq_len)\n",
    "    return attention[idToken, :]\n",
    "\n",
    "def weight_horizontal_token(attention, idToken, tokens):\n",
    "    indices = [i for i, token in enumerate(tokens) if token == tokens[idToken]]\n",
    "    return attention[indices, :].mean(dim=0)\n",
    "\n",
    "def weight_horizontal_mean(attention):\n",
    "    # attention : (seq_len, seq_len)\n",
    "    # output : tenseur de dimension (seq_len)\n",
    "    return attention.mean(dim=0)\n",
    "\n",
    "def weight_horizontal_PCA(attention):\n",
    "    # attention : (seq_len, seq_len)\n",
    "    # Chaque colonne est un échantillon, chaque ligne une feature\n",
    "    return PCA_on_dim_n(attention, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ceae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tensor(tensor):\n",
    "    norm = torch.linalg.norm(tensor)\n",
    "    return tensor / norm\n",
    "\n",
    "def normalize_tensor_col(tensor):\n",
    "    # tensor contenant les normes de chaque colonne de la matrice tensor\n",
    "    norm = torch.linalg.norm(tensor, dim=0, keepdim=True)\n",
    "    # on divise chaque colonne par sa norme\n",
    "    return tensor / norm\n",
    "\n",
    "def normalize_tensor_row(tensor):\n",
    "    norm = torch.linalg.norm(tensor, dim=1, keepdim=True)\n",
    "    # on divise chaque ligne par sa norme\n",
    "    return tensor / norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86a6452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtre_id_tensor(tensor,dim,id_start,id_end=0):\n",
    "    #sur la dimension spécifiée, ne garde que les indices entre id_start et id_end et met le reste à 0 (donc sans réduction de dimension)\n",
    "    if id_end == 0:\n",
    "        id_end = tensor.size(dim)\n",
    "    indices = torch.arange(tensor.size(dim), device=tensor.device)\n",
    "    mask_dim = (indices >= id_start) & (indices <= id_end)\n",
    "    shape = [1] * tensor.dim()\n",
    "    shape[dim] = -1\n",
    "    mask = mask_dim.view(shape)\n",
    "    return tensor * mask\n",
    "\n",
    "def reduce_id_tensor(tensor,dim,id_start,id_end=0):\n",
    "    # sur la dimension spécifiée, ne garde que les indices entre id_start et id_end, et réduit la dimension\n",
    "    if id_end == 0:\n",
    "        id_end = tensor.size(dim)\n",
    "    return tensor.narrow(dim, id_start, id_end - id_start)\n",
    "\n",
    "def idee_verticale(attentions, tokens, id_fin_prompt):\n",
    "    \"\"\"idToken = -1\n",
    "    indices = [i for i, token in enumerate(tokens) if token == tokens[idToken]]\n",
    "    filtre_indices = attentions\n",
    "    print(\"Shape of filtre_indices:\", filtre_indices.shape)\"\"\"\n",
    "    projection1 = reduce_id_tensor(attentions, dim=-2, id_start=id_fin_prompt)\n",
    "    projection1 = PCA_on_dim_n(projection1, dim=-1,n_espace_propre=0)\n",
    "    #projection1 = projection1[:,:,:,-1]\n",
    "    print(\"Shape of projection1:\", projection1.shape)\n",
    "    projection2 = projection1.sum(dim=1)\n",
    "    print(\"Shape of projection2:\", projection2.shape)\n",
    "    projection3 = projection2.sum(dim=0)\n",
    "    print(\"Shape of projection3:\", projection3.shape)\n",
    "    return projection3\n",
    "\n",
    "def idee_horizontale(attentions, tokens, id_fin_prompt):\n",
    "    \"\"\"idToken = -1\n",
    "    indices = [i for i, token in enumerate(tokens) if token == tokens[idToken]]\n",
    "    filtre_indices = attentions\n",
    "    print(\"Shape of filtre_indices:\", filtre_indices.shape)\"\"\"\n",
    "    projection1 = reduce_id_tensor(attentions, dim=-1, id_start=id_fin_prompt)\n",
    "    #projection1 = PCA_on_dim_n(projection1, dim=-2,n_espace_propre=1)\n",
    "    projection1 = somme_pondéré_PCA_valeur_propre(projection1,dim=-2,n_espace_propre_max=4)\n",
    "    #projection1 = projection1.sum(dim=-2)\n",
    "    print(\"Shape of projection1:\", projection1.shape)\n",
    "    projection2 = projection1.sum(dim=1)\n",
    "    print(\"Shape of projection2:\", projection2.shape)\n",
    "    projection3 = projection2.sum(dim=0)\n",
    "    print(\"Shape of projection3:\", projection3.shape)\n",
    "    return projection3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24967e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lyrics(df,index):\n",
    "    return df.loc[index, 'english_lyrics']\n",
    "\n",
    "def get_prompt_for_line(df,index, troncage = -1, tokenizer = None):\n",
    "    if troncage > 0:\n",
    "        lyrics = get_lyrics(df,index)\n",
    "        tokens = tokenizer.encode(lyrics)\n",
    "        tokens = tokens[:troncage]\n",
    "        lyrics = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "        return prompt_gender(lyrics)\n",
    "    return prompt_gender(get_lyrics(df,index))\n",
    "\n",
    "def load_attention_and_prompt_from_index(index, next = False):\n",
    "    Path_tensor_gender = f\"/home/evuichard/Projet DEBIAR/labeled_lyrics_tensors_{modele_str}/gender/\" + f\"/line_{index}.pt\"\n",
    "    data_tensor = torch.load(Path_tensor_gender)\n",
    "    if next:\n",
    "        attention = torch.stack(data_tensor['attentions_next'])[:,0,:,:,:]\n",
    "        print(\"Shape of attention (next):\", attention.shape)\n",
    "        return attention, data_tensor['generated_text']\n",
    "    attention = torch.stack(data_tensor['attentions_prompt'])[:,0,:,:,:]\n",
    "    prompt = data_tensor['generated_text']\n",
    "    c = prompt[-1]\n",
    "    prompt = prompt[:-1]\n",
    "    while prompt and c != ' ' and c != ':':\n",
    "        c = prompt[-1]\n",
    "        prompt = prompt[:-1]\n",
    "    return attention, prompt\n",
    "\n",
    "def attentionsToAttention(attentions):\n",
    "    #reduction layer\n",
    "    #attention = attentions.mean(dim=0)\n",
    "    attention = attentions[-1]\n",
    "    #attention = PCA_on_dim_n(attentions)\n",
    "    print(\"Shape of attention (layer):\", attention.shape)\n",
    "    #reduction head\n",
    "    #attention = attention.mean(dim=0)\n",
    "    #attention = attention[-1]\n",
    "    attention = PCA_on_dim_n(attention)\n",
    "    print(\"Shape of attention (head):\", attention.shape)\n",
    "    return attention\n",
    "\n",
    "def visualize_attention_from_index_and_idToken(index, tokenizer):\n",
    "    attentions, prompt = load_attention_and_prompt_from_index(index, next=True)\n",
    "    attention = attentionsToAttention(attentions)\n",
    "    print(\"Shape of attention:\", attention.shape)\n",
    "\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    print(\"Shape of tokens:\", len(tokens))\n",
    "\n",
    "    weights = weight_horizontal_token(attention,-1,tokens)\n",
    "    print(\"Shape of weights (horizontal, idToken):\", weights.shape)\n",
    "    visualize_weight_sentence(tokenizer, tokens, weights)\n",
    "\n",
    "    weights = weight_vertical_token(attention,-1,tokens)\n",
    "    print(\"Shape of weights (vertical, idToken):\", weights.shape)\n",
    "    visualize_weight_sentence(tokenizer, tokens, weights)\n",
    "\n",
    "\n",
    "    visualize_attention_html(tokenizer, attention, tokens)\n",
    "\n",
    "df = pd.read_excel('/home/evuichard/Projet DEBIAR/translated_lyrics.xlsx', sheet_name=\"Sheet1\")\n",
    "\n",
    "#visualize_attention_from_index_and_idToken(108, tokenizer)\n",
    "\n",
    "def visualize_attention_idee(index,tokenizer):\n",
    "    attentions, prompt = load_attention_and_prompt_from_index(index, next=False)\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    len_prompt = 263 #len(tokenizer.encode(prompt_gender(\"\")))\n",
    "\n",
    "    weight_horizontal = idee_horizontale(attentions, tokens, len_prompt)\n",
    "    visualize_weight_sentence(tokenizer, tokens[len_prompt:], weight_horizontal)\n",
    "    print(\"Most important tokens (horizontal):\", get_most_important_tokens(tokenizer, tokens[len_prompt:], weight_horizontal))\n",
    "\n",
    "    weight_vertical = idee_verticale(attentions, tokens, len_prompt)\n",
    "    visualize_weight_sentence(tokenizer, tokens[len_prompt:], weight_vertical)\n",
    "    print(\"Most important tokens (vertical):\", get_most_important_tokens(tokenizer, tokens[len_prompt:], weight_vertical))\n",
    "\n",
    "visualize_attention_idee(90, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d94205",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_output_gender = f'/home/evuichard/Projet DEBIAR/labeled_lyrics_gender_{modele_str}.xlsx'\n",
    "df = pd.ExcelFile(PATH_output_gender)\n",
    "df = df.parse(\"Sheet1\")\n",
    "\n",
    "df['nb_token'] = df['english_lyrics'].apply(lambda x: len(tokenizer.encode(x)))\n",
    "#moyenne\n",
    "print(\"Moyenne du nombre de tokens par ligne :\", df['nb_token'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86febfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hidden_prompt_and_gender_from_index(tokenizer, index, next = False):\n",
    "    Path_tensor_gender = f\"/home/evuichard/Projet DEBIAR/labeled_lyrics_tensors_{modele_str}/gender/\" + f\"/line_{index}.pt\"\n",
    "    data_tensor = torch.load(Path_tensor_gender)\n",
    "    prompt = data_tensor['generated_text']\n",
    "    tokens = tokenizer.encode(prompt)[260:]\n",
    "    if not next:\n",
    "        tokens = tokens[:-1]\n",
    "    #print(\"Shape of tokens:\", len(tokens))\n",
    "    prompt = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "    if next: #'hidden_states_next'\n",
    "        hidden_states = data_tensor['hidden_states_next'][0]\n",
    "        print(\"Shape of hidden_states (next):\", hidden_states.shape)\n",
    "        return hidden_states, prompt, data_tensor['genre']\n",
    "    hidden_states = data_tensor['hidden_states_prompt'][0]\n",
    "    print(\"Shape of hidden_states (prompt):\", hidden_states.shape)\n",
    "    return hidden_states, prompt, data_tensor['genre']\n",
    "\n",
    "load_hidden_prompt_and_gender_from_index(tokenizer, 0, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290d19c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_1(tokenizer):\n",
    "    list_tokens_lyrics = []\n",
    "    list_gender = []\n",
    "    list_prompt = []\n",
    "    for index in range(0, 258):#258\n",
    "        hidden_states, prompt, genre = load_hidden_prompt_and_gender_from_index(tokenizer, index, False)\n",
    "        token_lyrics = hidden_states\n",
    "        list_tokens_lyrics.append(token_lyrics)\n",
    "        list_gender.append(genre)\n",
    "        list_prompt.append(prompt)\n",
    "        print(index, end=' ')\n",
    "    #tensor_lyrics = torch.stack(list_tokens_lyrics)\n",
    "    #print(\"Shape of tensor_lyrics:\", tensor_lyrics.shape)\n",
    "    return list_tokens_lyrics, list_gender, list_prompt\n",
    "\n",
    "list_tokens_lyrics, list_gender, list_prompt = test_1(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804a0710",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size_prompt = max([t.shape[0] for t in list_tokens_lyrics])\n",
    "print(\"Taille maximale des prompts :\", max_size_prompt)\n",
    "s_max = 308\n",
    "\n",
    "for i in range(len(list_tokens_lyrics)):\n",
    "    s = list_tokens_lyrics[i].shape[0]\n",
    "    if s != s_max:\n",
    "        ajusted_tensor = torch.stack([list_tokens_lyrics[i][(j*s)//s_max,:] for j in range(s_max)])\n",
    "        list_tokens_lyrics[i] = ajusted_tensor\n",
    "\n",
    "tensor_tokens_lyrics = torch.stack(list_tokens_lyrics)\n",
    "print(\"Shape of tensor_tokens_lyrics:\", tensor_tokens_lyrics.shape)\n",
    "PATH_output_gender = f\"/home/evuichard/Projet DEBIAR/labeled_lyrics_tensors_{modele_str}/\"\n",
    "\n",
    "#save tensor_lyrics\n",
    "torch.save(tensor_tokens_lyrics, PATH_output_gender + 'tensor_lyrics_gender_prompt_tokens_hiddens_states_on_last_layer.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a22de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "PATH_output_gender = f\"/home/evuichard/Projet DEBIAR/labeled_lyrics_tensors_{modele_str}/\"\n",
    "\n",
    "#save tensor_lyrics\n",
    "#torch.save(tensor_lyrics, PATH_output_gender + 'tensor_lyrics_gender_prompt_token-1.pt')\n",
    "# On applique PCA sur la deuxième dimension du tenseur tensor_lyrics pour réduire 6144 au 3 premières composantes PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "tensor_lyrics_pca = pca.fit_transform(tensor_lyrics.detach().numpy())\n",
    "variances = pca.explained_variance_ratio_\n",
    "print(f\"Variance expliquée par les 3 premières composantes PCA : {variances}\")\n",
    "\n",
    "#on récupère les vecteurs des axes PCA pour pouvoir réappliquer la même transformation\n",
    "pca_vectors = pca.components_\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# affichage des variances expliquées\n",
    "plt.figure()\n",
    "plt.bar(range(1, 4), variances, tick_label=['PCA 1', 'PCA 2', 'PCA 3'])\n",
    "plt.ylabel('Variance expliquée')\n",
    "plt.title('Variance expliquée par les 3 premières composantes PCA')\n",
    "plt.savefig(PATH_output_gender + 'PCA_variance_explained_lyrics1.png')\n",
    "plt.show()\n",
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"PCA1\": tensor_lyrics_pca[:, 0],\n",
    "    \"PCA2\": tensor_lyrics_pca[:, 1],\n",
    "    \"PCA3\": tensor_lyrics_pca[:, 2],\n",
    "    \"Gender\": list_gender\n",
    "})\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    df, x=\"PCA1\", y=\"PCA2\", z=\"PCA3\", color=\"Gender\",\n",
    "    color_discrete_map={\"male\": \"blue\", \"female\": \"red\", \"neutral\": \"green\"},\n",
    "    opacity=0.7\n",
    ")\n",
    "fig.update_traces(marker=dict(size=4)) \n",
    "fig.show()\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# affichage 2D\n",
    "plt.figure()\n",
    "plt.scatter(tensor_lyrics_pca[:, 0], tensor_lyrics_pca[:, 1], c=[colors[gender] for gender in list_gender])\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.title('Projection des données sur les 2 premières composantes PCA')\n",
    "plt.savefig(PATH_output_gender + 'PCA_2D_projection_lyrics1.png')\n",
    "plt.legend(handles=[plt.Line2D([0], [0], marker='o', color='w', label='male', markerfacecolor='blue'),\n",
    "                    plt.Line2D([0], [0], marker='o', color='w', label='female', markerfacecolor='red'),\n",
    "                    plt.Line2D([0], [0], marker='o', color='w', label='neutral', markerfacecolor='green')])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd25f170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Paramètres ####\n",
    "index = 0\n",
    "next = False  # True pour le token suivant, False pour le prompt\n",
    "\n",
    "hidden_states, prompt, genre = load_hidden_prompt_and_gender_from_index(tokenizer, index, next)\n",
    "print(f\"Hidden states: {hidden_states.shape}\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Genre: {genre}\")\n",
    "\n",
    "#Hidden states shape = [308,6144]\n",
    "# on réapplique les vecteurs PCA\n",
    "hidden_states_pca = hidden_states @ pca_vectors[:].T\n",
    "print(f\"Hidden states PCA shape: {hidden_states_pca.shape}\")\n",
    "\n",
    "v = np.arange(hidden_states_pca.shape[0])\n",
    "# v comme nouveau dimension de hidden_states_pca pour pouvoir afficher l'évolution ensuite en 3d\n",
    "# vizual_vector must be in dimension 3\n",
    "vizual_vector = np.concatenate((v[:, np.newaxis], hidden_states_pca), axis=1)\n",
    "print(f\"Vizual vector shape: {vizual_vector.shape}\")\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(vizual_vector, columns=[\"Step\"] + [f\"PC{i+1}\" for i in range(hidden_states_pca.shape[1])])\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=df[\"PC1\"],\n",
    "    y=df[\"PC2\"],\n",
    "    z=df[\"Step\"],\n",
    "    mode='lines+markers',\n",
    "    marker=dict(\n",
    "        size=3,\n",
    "        color=df[\"Step\"],  # Couleur évolue selon Step\n",
    "        colorscale='Viridis',  # Palette de couleurs\n",
    "        colorbar=dict(title='Step')\n",
    "    ),\n",
    "    line=dict(width=2)\n",
    ")])\n",
    "fig.update_layout(scene=dict(\n",
    "    xaxis_title='PC1',\n",
    "    yaxis_title='PC2',\n",
    "    zaxis_title='Step'\n",
    "))\n",
    "fig.show()\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"PC1\": hidden_states_pca[:, 0],\n",
    "    \"PC2\": hidden_states_pca[:, 1],\n",
    "    \"Step\": np.arange(hidden_states_pca.shape[0])\n",
    "})\n",
    "\n",
    "fig = px.scatter(df, x=\"PC1\", y=\"PC2\",\n",
    "                 animation_frame=\"Step\",\n",
    "                 range_x=[df[\"PC1\"].min()-1, df[\"PC1\"].max()+1],\n",
    "                 range_y=[df[\"PC2\"].min()-1, df[\"PC2\"].max()+1],\n",
    "                 color=\"Step\",\n",
    "                 color_continuous_scale=\"Viridis\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"PC1\": hidden_states_pca[:, 0],\n",
    "    \"PC2\": hidden_states_pca[:, 1],\n",
    "    \"PC3\": hidden_states_pca[:, 2],\n",
    "    \"Step\": np.arange(hidden_states_pca.shape[0])\n",
    "})\n",
    "\n",
    "# Création de la figure avec le premier point\n",
    "fig = go.Figure(\n",
    "    data=[go.Scatter3d(\n",
    "        x=[df.loc[0, \"PC1\"]],\n",
    "        y=[df.loc[0, \"PC2\"]],\n",
    "        z=[df.loc[0, \"PC3\"]],\n",
    "        mode=\"markers+lines\",\n",
    "        marker=dict(size=4, color=\"blue\"),\n",
    "        line=dict(color=\"blue\", width=2)\n",
    "    )]\n",
    ")\n",
    "\n",
    "# Ajout des frames (1 par step)\n",
    "frames = []\n",
    "for i in range(1, len(df)):\n",
    "    frames.append(go.Frame(\n",
    "        data=[go.Scatter3d(\n",
    "            x=df.loc[:i, \"PC1\"],\n",
    "            y=df.loc[:i, \"PC2\"],\n",
    "            z=df.loc[:i, \"PC3\"],\n",
    "            mode=\"markers+lines\",\n",
    "            marker=dict(size=4, color=df.loc[:i, \"Step\"], colorscale=\"Viridis\"),\n",
    "            line=dict(color=\"blue\", width=2)\n",
    "        )],\n",
    "        name=str(i)\n",
    "    ))\n",
    "\n",
    "fig.frames = frames\n",
    "\n",
    "# Boutons de contrôle de l'animation\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title=\"PC1\",\n",
    "        yaxis_title=\"PC2\",\n",
    "        zaxis_title=\"PC3\"\n",
    "    ),\n",
    "    updatemenus=[dict(\n",
    "        type=\"buttons\",\n",
    "        showactive=False,\n",
    "        buttons=[\n",
    "            dict(label=\"▶ Play\",\n",
    "                 method=\"animate\",\n",
    "                 args=[None, dict(frame=dict(duration=50, redraw=True), fromcurrent=True)]),\n",
    "            dict(label=\"⏸ Pause\",\n",
    "                 method=\"animate\",\n",
    "                 args=[[None], dict(frame=dict(duration=0, redraw=False), mode=\"immediate\")])\n",
    "        ]\n",
    "    )]\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8a526f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shape of tensor_tokens_lyrics: torch.Size([258, 308, 6144])\n",
    "# 258 lyrics, 308 tokens, 6144 embedding dimensions\n",
    "print(f\"Shape of tensor_tokens_lyrics: {tensor_tokens_lyrics.shape}\")\n",
    "#on fait une PCA pour diminuer le nb de dimensions embeddings à 3 pour chaque token\n",
    "tensor_tokens_lyrics_pca = torch.empty((tensor_tokens_lyrics.shape[0], tensor_tokens_lyrics.shape[1], 3), device=tensor_tokens_lyrics.device)  \n",
    "variances = []\n",
    "for i in range(tensor_tokens_lyrics.shape[1]):\n",
    "    pca = PCA(n_components=3)\n",
    "    pca_result = pca.fit_transform(tensor_tokens_lyrics[:, i, :].reshape(-1, 6144)).reshape(tensor_tokens_lyrics.shape[0], 3)\n",
    "    tensor_tokens_lyrics_pca[:, i, :] = torch.from_numpy(pca_result).to(tensor_tokens_lyrics_pca.device)\n",
    "    variances.append(pca.explained_variance_ratio_)\n",
    "print(f\"Shape of tensor_tokens_lyrics_pca: {tensor_tokens_lyrics_pca.shape}\")\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# tensor_tokens_lyrics_pca : shape [258, 308, 3]\n",
    "# list_gender : 258 labels (\"male\", \"female\", \"neutral\")\n",
    "\n",
    "# On prépare un DataFrame long format\n",
    "data = []\n",
    "for step in range(tensor_tokens_lyrics_pca.shape[1]):\n",
    "    for i in range(tensor_tokens_lyrics_pca.shape[0]):\n",
    "        data.append({\n",
    "            \"PC1\": tensor_tokens_lyrics_pca[i, step, 0].item(),\n",
    "            \"PC2\": tensor_tokens_lyrics_pca[i, step, 1].item(),\n",
    "            \"PC3\": tensor_tokens_lyrics_pca[i, step, 2].item(),\n",
    "            \"Step\": step,\n",
    "            \"Gender\": list_gender[i]\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Mapping couleurs\n",
    "color_map = {\"male\": \"blue\", \"female\": \"red\", \"neutral\": \"green\"}\n",
    "\n",
    "# Création de la figure\n",
    "fig = go.Figure(\n",
    "    data=[go.Scatter3d(\n",
    "        x=df.loc[df[\"Step\"] == 0, \"PC1\"],\n",
    "        y=df.loc[df[\"Step\"] == 0, \"PC2\"],\n",
    "        z=df.loc[df[\"Step\"] == 0, \"PC3\"],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=3, color=[color_map[g] for g in df.loc[df[\"Step\"] == 0, \"Gender\"]])\n",
    "    )]\n",
    ")\n",
    "\n",
    "# Ajout des frames (une frame = un step/token)\n",
    "frames = []\n",
    "for step in range(1, tensor_tokens_lyrics_pca.shape[1]):\n",
    "    frames.append(go.Frame(\n",
    "        data=[go.Scatter3d(\n",
    "            x=df.loc[df[\"Step\"] == step, \"PC1\"],\n",
    "            y=df.loc[df[\"Step\"] == step, \"PC2\"],\n",
    "            z=df.loc[df[\"Step\"] == step, \"PC3\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=3, color=[color_map[g] for g in df.loc[df[\"Step\"] == step, \"Gender\"]])\n",
    "        )],\n",
    "        name=str(step)\n",
    "    ))\n",
    "\n",
    "fig.frames = frames\n",
    "\n",
    "# Boutons de contrôle\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title=\"PC1\",\n",
    "        yaxis_title=\"PC2\",\n",
    "        zaxis_title=\"PC3\"\n",
    "    ),\n",
    "    updatemenus=[dict(\n",
    "        type=\"buttons\",\n",
    "        showactive=False,\n",
    "        buttons=[\n",
    "            dict(label=\"▶ Play\",\n",
    "                 method=\"animate\",\n",
    "                 args=[None, dict(frame=dict(duration=100, redraw=True), fromcurrent=True)]),\n",
    "            dict(label=\"⏸ Pause\",\n",
    "                 method=\"animate\",\n",
    "                 args=[[None], dict(frame=dict(duration=0, redraw=False), mode=\"immediate\")])\n",
    "        ]\n",
    "    )],\n",
    "    width=900,   # largeur en pixels\n",
    "    height=700   # hauteur en pixels\n",
    ")\n",
    "fig.to_html(\"3d_scatter_timelapse_gender_prompt_evolution_of_tokens_embeddings_over_tokens.html\")\n",
    "fig.show()\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "# tensor_tokens_lyrics_pca : shape [258, 308, 3]\n",
    "# list_gender : 258 labels (\"male\", \"female\", \"neutral\")\n",
    "# variances : liste de np.array shape (3,), une par step\n",
    "\n",
    "# Préparation des données en DataFrame\n",
    "data = []\n",
    "for step in range(tensor_tokens_lyrics_pca.shape[1]):\n",
    "    for i in range(tensor_tokens_lyrics_pca.shape[0]):\n",
    "        data.append({\n",
    "            \"PC1\": tensor_tokens_lyrics_pca[i, step, 0].item(),\n",
    "            \"PC2\": tensor_tokens_lyrics_pca[i, step, 1].item(),\n",
    "            \"PC3\": tensor_tokens_lyrics_pca[i, step, 2].item(),\n",
    "            \"Step\": step,\n",
    "            \"Gender\": list_gender[i]\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Couleurs selon genre\n",
    "color_map = {\"male\": \"blue\", \"female\": \"red\", \"neutral\": \"green\"}\n",
    "\n",
    "# Fonction pour générer un titre d’axes avec variances\n",
    "def axis_titles(step):\n",
    "    v = variances[step] * 100  # si c’est en proportions, *100 pour %\n",
    "    return dict(\n",
    "        xaxis_title=f\"PC1 ({v[0]:.1f}%)\",\n",
    "        yaxis_title=f\"PC2 ({v[1]:.1f}%)\",\n",
    "        zaxis_title=f\"PC3 ({v[2]:.1f}%)\"\n",
    "    )\n",
    "\n",
    "# Figure initiale (Step = 0)\n",
    "fig = go.Figure(\n",
    "    data=[go.Scatter3d(\n",
    "        x=df.loc[df[\"Step\"] == 0, \"PC1\"],\n",
    "        y=df.loc[df[\"Step\"] == 0, \"PC2\"],\n",
    "        z=df.loc[df[\"Step\"] == 0, \"PC3\"],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=3, color=[color_map[g] for g in df.loc[df[\"Step\"] == 0, \"Gender\"]])\n",
    "    )]\n",
    ")\n",
    "\n",
    "# Frames = un step = un nuage de points + axes mis à jour\n",
    "frames = []\n",
    "for step in range(tensor_tokens_lyrics_pca.shape[1]):\n",
    "    frames.append(go.Frame(\n",
    "        data=[go.Scatter3d(\n",
    "            x=df.loc[df[\"Step\"] == step, \"PC1\"],\n",
    "            y=df.loc[df[\"Step\"] == step, \"PC2\"],\n",
    "            z=df.loc[df[\"Step\"] == step, \"PC3\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=3, color=[color_map[g] for g in df.loc[df[\"Step\"] == step, \"Gender\"]])\n",
    "        )],\n",
    "        layout=dict(scene=axis_titles(step)),\n",
    "        name=str(step)\n",
    "    ))\n",
    "fig.frames = frames\n",
    "\n",
    "# Ajout du slider\n",
    "steps = []\n",
    "for step in range(len(frames)):\n",
    "    step_dict = dict(\n",
    "        method=\"animate\",\n",
    "        args=[[str(step)], dict(mode=\"immediate\", frame=dict(duration=0, redraw=True), transition=dict(duration=0))],\n",
    "        label=str(step)\n",
    "    )\n",
    "    steps.append(step_dict)\n",
    "\n",
    "sliders = [dict(\n",
    "    active=0,\n",
    "    currentvalue={\"prefix\": \"Step: \"},\n",
    "    pad={\"t\": 50},\n",
    "    steps=steps\n",
    ")]\n",
    "\n",
    "fig.update_layout(\n",
    "    scene=axis_titles(0),  # initial avec step 0\n",
    "    sliders=sliders,\n",
    "    width=900,   # largeur en pixels\n",
    "    height=700   # hauteur en pixels\n",
    ")\n",
    "\n",
    "fig.to_html(\"3d_scatter_slider_gender_prompt_evolution_of_tokens_embeddings_over_tokens.html\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "KXQltZsTDd63",
    "JmBmko5YDI0g"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (GPU ENV)",
   "language": "python",
   "name": "gpuenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
